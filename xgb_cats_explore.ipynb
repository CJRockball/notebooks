{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ea947d",
   "metadata": {},
   "source": [
    "# Categorical Feature Encoding Analysis: XGBoost Performance Comparison\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This comprehensive analysis compares four different categorical encoding methods using XGBoost classifier on a bank marketing dataset. **Weight of Evidence (WOE) encoding emerged as the top performer**, achieving the highest ROC-AUC score (0.96687) and lowest log loss (0.14694), followed closely by Target Encoding. The analysis demonstrates that while all encoding methods produced strong results, target-aware encoding techniques (WOE and Target Encoding) consistently outperformed traditional methods.\n",
    "\n",
    "## Introduction to Categorical Feature Encoding\n",
    "\n",
    "Categorical feature encoding is a crucial preprocessing step in machine learning that transforms non-numerical categorical variables into numerical representations that algorithms can process effectively. The choice of encoding method significantly impacts model performance, training time, and interpretability.\n",
    "\n",
    "### Key Encoding Methods Analyzed\n",
    "\n",
    "**Label Encoding** assigns sequential integers to each unique category. While simple and memory-efficient, it can introduce unintended ordinal relationships between categories that don't naturally exist.\n",
    "\n",
    "**One-Hot Encoding** creates binary dummy variables for each category, ensuring no ordinal relationships are imposed. However, it can lead to high dimensionality and sparse feature spaces, particularly problematic with high-cardinality categorical variables.\n",
    "\n",
    "**Target Encoding** replaces categories with the mean of the target variable for that category. This creates a direct relationship between the categorical feature and the target, but requires careful implementation to prevent data leakage and overfitting.\n",
    "\n",
    "**Weight of Evidence (WOE) Encoding** transforms categories based on the natural logarithm of the ratio of positive to negative target outcomes. Originally developed for credit scoring, WOE provides a monotonic relationship with the target variable and is robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e7addf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load requirements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from category_encoders import TargetEncoder, WOEEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a334c066",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "The analysis utilized a bank marketing dataset used in the Kaggle playground competition, \"Bianry Classification with a Babnking Dataset\". It's labeled S5E8 and started on 01/08/2025, [get it here](https://www.kaggle.com/competitions/playground-series-s5e8/data). The dataset contains 750,000 training samples and 250,000 test samples with 17 features total. The dataset includes **9 categorical features**: job, marital status, education, default status, housing loan, personal loan, contact method, month, and previous campaign outcome, although it can be modelled differently. The target variable is binary, representing whether a customer subscribed to a term deposit.\n",
    "\n",
    "The dataset showed **no missing values**, eliminating the need for imputation strategies and allowing for a clean comparison of encoding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60e445fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>7</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>25</td>\n",
       "      <td>aug</td>\n",
       "      <td>117</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>514</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>18</td>\n",
       "      <td>jun</td>\n",
       "      <td>185</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>602</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>14</td>\n",
       "      <td>may</td>\n",
       "      <td>111</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>student</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>34</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>28</td>\n",
       "      <td>may</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>889</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>3</td>\n",
       "      <td>feb</td>\n",
       "      <td>902</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749995</th>\n",
       "      <td>29</td>\n",
       "      <td>services</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>1282</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>4</td>\n",
       "      <td>jul</td>\n",
       "      <td>1006</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749996</th>\n",
       "      <td>69</td>\n",
       "      <td>retired</td>\n",
       "      <td>divorced</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>631</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>19</td>\n",
       "      <td>aug</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749997</th>\n",
       "      <td>50</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>217</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>17</td>\n",
       "      <td>apr</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749998</th>\n",
       "      <td>32</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>-274</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>26</td>\n",
       "      <td>aug</td>\n",
       "      <td>108</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749999</th>\n",
       "      <td>42</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>1559</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>4</td>\n",
       "      <td>aug</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>failure</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>750000 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age          job   marital  education default  balance housing loan  \\\n",
       "0        42   technician   married  secondary      no        7      no   no   \n",
       "1        38  blue-collar   married  secondary      no      514      no   no   \n",
       "2        36  blue-collar   married  secondary      no      602     yes   no   \n",
       "3        27      student    single  secondary      no       34     yes   no   \n",
       "4        26   technician   married  secondary      no      889     yes   no   \n",
       "...     ...          ...       ...        ...     ...      ...     ...  ...   \n",
       "749995   29     services    single  secondary      no     1282      no  yes   \n",
       "749996   69      retired  divorced   tertiary      no      631      no   no   \n",
       "749997   50  blue-collar   married  secondary      no      217     yes   no   \n",
       "749998   32   technician   married  secondary      no     -274      no   no   \n",
       "749999   42   technician   married  secondary      no     1559      no   no   \n",
       "\n",
       "         contact  day month  duration  campaign  pdays  previous poutcome  y  \n",
       "0       cellular   25   aug       117         3     -1         0  unknown  0  \n",
       "1        unknown   18   jun       185         1     -1         0  unknown  0  \n",
       "2        unknown   14   may       111         2     -1         0  unknown  0  \n",
       "3        unknown   28   may        10         2     -1         0  unknown  0  \n",
       "4       cellular    3   feb       902         1     -1         0  unknown  1  \n",
       "...          ...  ...   ...       ...       ...    ...       ...      ... ..  \n",
       "749995   unknown    4   jul      1006         2     -1         0  unknown  1  \n",
       "749996  cellular   19   aug        87         1     -1         0  unknown  0  \n",
       "749997  cellular   17   apr       113         1     -1         0  unknown  0  \n",
       "749998  cellular   26   aug       108         6     -1         0  unknown  0  \n",
       "749999  cellular    4   aug       143         1      1         7  failure  0  \n",
       "\n",
       "[750000 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "df_train = pd.read_csv('data/raw/train.csv').drop(columns=['id'])\n",
    "df_test = pd.read_csv('data/raw/test.csv').drop(columns=['id'])\n",
    "\n",
    "display(df_train)\n",
    "\n",
    "target = ['y']\n",
    "cats = ['job','marital','education','default','housing','loan','contact','month','poutcome']\n",
    "nums = ['age', 'balance','day', 'duration', 'capaign','pdays']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603d4eef",
   "metadata": {},
   "source": [
    "For an initial test of the dataset I use XGB. XGB is very forgiving and you can run almost any type of dataset. However, I like to impute nans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86bfe818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age          0\n",
      "job          0\n",
      "marital      0\n",
      "education    0\n",
      "default      0\n",
      "balance      0\n",
      "housing      0\n",
      "loan         0\n",
      "contact      0\n",
      "day          0\n",
      "month        0\n",
      "duration     0\n",
      "campaign     0\n",
      "pdays        0\n",
      "previous     0\n",
      "poutcome     0\n",
      "y            0\n",
      "dtype: int64\n",
      "age          0\n",
      "job          0\n",
      "marital      0\n",
      "education    0\n",
      "default      0\n",
      "balance      0\n",
      "housing      0\n",
      "loan         0\n",
      "contact      0\n",
      "day          0\n",
      "month        0\n",
      "duration     0\n",
      "campaign     0\n",
      "pdays        0\n",
      "previous     0\n",
      "poutcome     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.isnull().sum())\n",
    "print(df_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6505f272",
   "metadata": {},
   "source": [
    "## Methodology and Implementation\n",
    "\n",
    "### Cross-Validation Strategy\n",
    "All experiments employed **5-fold cross-validation** with consistent random seeding (1337) to ensure reproducible and fair comparisons across encoding methods.\n",
    "\n",
    "### Model Configuration\n",
    "XGBoost classifier was selected for its robustness and ability to handle various data types effectively. The model configuration included:\n",
    "- Tree method: histogram-based for efficiency\n",
    "- Early stopping: 100 rounds with log loss monitoring\n",
    "- Maximum depth: 6 to prevent overfitting\n",
    "- Categorical support: enabled for appropriate handling\n",
    "\n",
    "### Encoding Implementation\n",
    "**Critical Implementation Detail**: Target Encoding and Weight of Evidence encoding were properly implemented within the cross-validation loop to prevent data leakage. The encoders were fitted only on training data and then applied to validation sets.\n",
    "\n",
    "For Target Encoding, smoothing parameters were carefully tuned:\n",
    "- `min_samples_leaf=20`: Minimum samples required for reliable encoding\n",
    "- `smoothing=10`: Regularization to prevent overfitting on rare categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b5cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Coding Algo': [], 'Number of Features': [], 'Average Logloss': [], 'Average ROC-AUC': []}\n"
     ]
    }
   ],
   "source": [
    "# Collect data in a dict of lists to summarize the experiment\n",
    "cols = ['Coding Algo', 'Number of Features', 'Average Logloss', 'Average ROC-AUC']\n",
    "summary_dict = {k:[] for k in cols}\n",
    "print(summary_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4ad5d9",
   "metadata": {},
   "source": [
    "Let's start modeling with my favorite, Label encoding. I start by replacing existing labels with number labels. For XGB we don't have to, it would take care of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24adb969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(750000, 17) (250000, 16)\n",
      "(750000, 17) (250000, 16)\n"
     ]
    }
   ],
   "source": [
    "def ord_trans(df, df1, cats):\n",
    "    train_len = len(df)\n",
    "    df_temp = pd.concat([df, df1], axis=0)\n",
    "    \n",
    "    for name in cats:\n",
    "        df_temp[name], _ = df_temp[name].factorize()\n",
    "        \n",
    "    df = df_temp.iloc[:train_len,:].copy()\n",
    "    df1 = df_temp.iloc[train_len:,:].copy()\n",
    "    df1 = df1.drop(columns=['y'])\n",
    "    return df, df1\n",
    "\n",
    "print(df_train.shape, df_test.shape)\n",
    "df_label_train, df_label_test = ord_trans(df_train, df_test, cats)\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55ee3ef",
   "metadata": {},
   "source": [
    "Use cross-validation make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9996fd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.26203\tvalidation_0-auc:0.93672\n",
      "[50]\tvalidation_0-logloss:0.15125\tvalidation_0-auc:0.96456\n",
      "[100]\tvalidation_0-logloss:0.14834\tvalidation_0-auc:0.96605\n",
      "[150]\tvalidation_0-logloss:0.14728\tvalidation_0-auc:0.96654\n",
      "[190]\tvalidation_0-logloss:0.14707\tvalidation_0-auc:0.96665\n",
      "Fold 1 Log Loss: 0.14707, AUC_ROC: 0.96665\n",
      "[0]\tvalidation_0-logloss:0.26362\tvalidation_0-auc:0.93599\n",
      "[50]\tvalidation_0-logloss:0.15177\tvalidation_0-auc:0.96481\n",
      "[100]\tvalidation_0-logloss:0.14893\tvalidation_0-auc:0.96621\n",
      "[150]\tvalidation_0-logloss:0.14784\tvalidation_0-auc:0.96673\n",
      "[164]\tvalidation_0-logloss:0.14765\tvalidation_0-auc:0.96680\n",
      "Fold 2 Log Loss: 0.14765, AUC_ROC: 0.96680\n",
      "[0]\tvalidation_0-logloss:0.26366\tvalidation_0-auc:0.93438\n",
      "[50]\tvalidation_0-logloss:0.15347\tvalidation_0-auc:0.96352\n",
      "[100]\tvalidation_0-logloss:0.15029\tvalidation_0-auc:0.96514\n",
      "[150]\tvalidation_0-logloss:0.14877\tvalidation_0-auc:0.96591\n",
      "[200]\tvalidation_0-logloss:0.14822\tvalidation_0-auc:0.96619\n",
      "[208]\tvalidation_0-logloss:0.14809\tvalidation_0-auc:0.96626\n",
      "Fold 3 Log Loss: 0.14809, AUC_ROC: 0.96626\n",
      "[0]\tvalidation_0-logloss:0.26214\tvalidation_0-auc:0.93459\n",
      "[50]\tvalidation_0-logloss:0.15057\tvalidation_0-auc:0.96483\n",
      "[100]\tvalidation_0-logloss:0.14781\tvalidation_0-auc:0.96621\n",
      "[150]\tvalidation_0-logloss:0.14694\tvalidation_0-auc:0.96668\n",
      "[169]\tvalidation_0-logloss:0.14651\tvalidation_0-auc:0.96687\n",
      "Fold 4 Log Loss: 0.14652, AUC_ROC: 0.96687\n",
      "[0]\tvalidation_0-logloss:0.26296\tvalidation_0-auc:0.93707\n",
      "[50]\tvalidation_0-logloss:0.15096\tvalidation_0-auc:0.96501\n",
      "[100]\tvalidation_0-logloss:0.14773\tvalidation_0-auc:0.96660\n",
      "[150]\tvalidation_0-logloss:0.14653\tvalidation_0-auc:0.96716\n",
      "[194]\tvalidation_0-logloss:0.14620\tvalidation_0-auc:0.96729\n",
      "Fold 5 Log Loss: 0.14620, AUC_ROC: 0.96729\n",
      "\n",
      "Overall Score, logloss: 0.14711, auc: 0.96677\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_y = df_label_train[['y']].copy()\n",
    "df_X = df_label_train.drop(columns=['y']).copy()\n",
    "Xtest = df_label_test.copy()\n",
    "\n",
    "KFOLD = 5\n",
    "kf = KFold(n_splits=KFOLD, shuffle=True, random_state=1337)\n",
    "\n",
    "fold_loglosses = []\n",
    "fold_metrics = []\n",
    "for i,(train_index, valid_index) in enumerate(kf.split(df_X)):\n",
    "    Xtrain = df_X.iloc[train_index]\n",
    "    ytrain = df_y.iloc[train_index]\n",
    "    Xvalid = df_X.iloc[valid_index]\n",
    "    yvalid = df_y.iloc[valid_index]\n",
    "    \n",
    "    # XGB    \n",
    "    # Early stopping call back, use to get best model back\n",
    "    es = xgb.callback.EarlyStopping(\n",
    "    rounds=50,\n",
    "    min_delta=1e-3,\n",
    "    save_best=True,\n",
    "    maximize=False,\n",
    "    data_name=\"validation_0\",\n",
    "    metric_name=\"logloss\",)\n",
    "    \n",
    "    model = XGBClassifier(tree_method='hist',\n",
    "                          n_estimators=2000, \n",
    "                          objective='binary:logistic',\n",
    "                          early_stopping_rounds=100, \n",
    "                          enable_categorical=True, \n",
    "                          eval_metric=['logloss', 'auc'],\n",
    "                          n_jobs=4,\n",
    "                          random_state=1337,\n",
    "                          callbacks=[es],\n",
    "                          \n",
    "                          max_depth = 6,)\n",
    "    \n",
    "    model = model.fit(Xtrain, ytrain, \n",
    "                      eval_set=[(Xvalid, yvalid)],\n",
    "                      verbose=50)\n",
    "    \n",
    "    # predict\n",
    "    ypred_proba = model.predict_proba(Xvalid)\n",
    "    ypred = model.predict(Xvalid)\n",
    "    fold_logloss = log_loss(yvalid, ypred_proba)\n",
    "    fold_metric = roc_auc_score(yvalid, ypred_proba[:,1])\n",
    "    \n",
    "    # save\n",
    "    fold_loglosses.append(fold_logloss)\n",
    "    fold_metrics.append(fold_metric)\n",
    "    print(f\"Fold {i+1} Log Loss: {fold_logloss:.5f}, AUC_ROC: {fold_metric:.5f}\")\n",
    "    \n",
    "\n",
    "print(f\"\\nOverall Score, logloss: {np.mean(fold_loglosses):.5f}, auc: {np.mean(fold_metrics):.5f}\")\n",
    "\n",
    "summary_list = ['Label', str(Xtrain.shape[1]), str(round(np.mean(fold_loglosses),5)), str(round(np.mean(fold_metrics),5))]\n",
    "for i, (k, v) in enumerate(summary_dict.items()):\n",
    "    v.append(summary_list[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc66b4f0",
   "metadata": {},
   "source": [
    "Try one-hot-encoding. get_dummies returns bool type which will be run as categorical in xgboos models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a55af85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of new cols:  26\n",
      "number of current cols:  (750000, 17)\n",
      "total new columns:  (750000, 43)\n",
      "should be 17 + 26 =  43\n",
      "new cols in test:  (250000, 42)\n",
      "should be:  42\n"
     ]
    }
   ],
   "source": [
    "new_cols = 0\n",
    "for name in cats:\n",
    "    new_cols += np.max(df_train[name].nunique() - 2,0)\n",
    "print('number of new cols: ', new_cols)\n",
    "\n",
    "print('number of current cols: ', df_train.shape)\n",
    "\n",
    "df_ohe_train = pd.get_dummies(df_train, prefix=cats, dummy_na=False, \n",
    "                           columns=cats, drop_first=True)\n",
    "df_ohe_test = pd.get_dummies(df_test, prefix=cats, columns=cats, drop_first=True)\n",
    "\n",
    "print('total new columns: ', df_ohe_train.shape)\n",
    "print('should be 17 + 26 = ', 17+26)\n",
    "print('new cols in test: ', df_ohe_test.shape)\n",
    "print('should be: ', df_test.shape[1] + 26)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fbd7e2",
   "metadata": {},
   "source": [
    "let make the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "926a0274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.26281\tvalidation_0-auc:0.93563\n",
      "[50]\tvalidation_0-logloss:0.15150\tvalidation_0-auc:0.96445\n",
      "[100]\tvalidation_0-logloss:0.14893\tvalidation_0-auc:0.96570\n",
      "[150]\tvalidation_0-logloss:0.14796\tvalidation_0-auc:0.96622\n",
      "[177]\tvalidation_0-logloss:0.14760\tvalidation_0-auc:0.96639\n",
      "Fold 1 Log Loss: 0.14759, AUC_ROC: 0.96640\n",
      "[0]\tvalidation_0-logloss:0.26467\tvalidation_0-auc:0.93516\n",
      "[50]\tvalidation_0-logloss:0.15322\tvalidation_0-auc:0.96408\n",
      "[100]\tvalidation_0-logloss:0.15011\tvalidation_0-auc:0.96559\n",
      "[150]\tvalidation_0-logloss:0.14844\tvalidation_0-auc:0.96646\n",
      "[175]\tvalidation_0-logloss:0.14831\tvalidation_0-auc:0.96655\n",
      "Fold 2 Log Loss: 0.14828, AUC_ROC: 0.96656\n",
      "[0]\tvalidation_0-logloss:0.26427\tvalidation_0-auc:0.93490\n",
      "[50]\tvalidation_0-logloss:0.15416\tvalidation_0-auc:0.96303\n",
      "[100]\tvalidation_0-logloss:0.15053\tvalidation_0-auc:0.96495\n",
      "[150]\tvalidation_0-logloss:0.14922\tvalidation_0-auc:0.96562\n",
      "[174]\tvalidation_0-logloss:0.14885\tvalidation_0-auc:0.96582\n",
      "Fold 3 Log Loss: 0.14885, AUC_ROC: 0.96582\n",
      "[0]\tvalidation_0-logloss:0.26329\tvalidation_0-auc:0.93216\n",
      "[50]\tvalidation_0-logloss:0.15220\tvalidation_0-auc:0.96402\n",
      "[100]\tvalidation_0-logloss:0.14860\tvalidation_0-auc:0.96583\n",
      "[150]\tvalidation_0-logloss:0.14738\tvalidation_0-auc:0.96640\n",
      "[193]\tvalidation_0-logloss:0.14703\tvalidation_0-auc:0.96659\n",
      "Fold 4 Log Loss: 0.14702, AUC_ROC: 0.96660\n",
      "[0]\tvalidation_0-logloss:0.26427\tvalidation_0-auc:0.93685\n",
      "[50]\tvalidation_0-logloss:0.15173\tvalidation_0-auc:0.96463\n",
      "[100]\tvalidation_0-logloss:0.14864\tvalidation_0-auc:0.96613\n",
      "[150]\tvalidation_0-logloss:0.14739\tvalidation_0-auc:0.96672\n",
      "[174]\tvalidation_0-logloss:0.14700\tvalidation_0-auc:0.96692\n",
      "Fold 5 Log Loss: 0.14699, AUC_ROC: 0.96692\n",
      "\n",
      "Overall Score, logloss: 0.14774, auc: 0.96646\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_y = df_ohe_train[['y']].copy()\n",
    "df_X = df_ohe_train.drop(columns=['y']).copy()\n",
    "Xtest = df_ohe_test.copy()\n",
    "\n",
    "KFOLD = 5\n",
    "kf = KFold(n_splits=KFOLD, shuffle=True, random_state=1337)\n",
    "\n",
    "fold_loglosses = []\n",
    "fold_metrics = []\n",
    "for i,(train_index, valid_index) in enumerate(kf.split(df_X)):\n",
    "    Xtrain = df_X.iloc[train_index]\n",
    "    ytrain = df_y.iloc[train_index]\n",
    "    Xvalid = df_X.iloc[valid_index]\n",
    "    yvalid = df_y.iloc[valid_index]\n",
    "    \n",
    "    # XGB    \n",
    "    # Early stopping call back, use to get best model back\n",
    "    es = xgb.callback.EarlyStopping(\n",
    "    rounds=50,\n",
    "    min_delta=1e-3,\n",
    "    save_best=True,\n",
    "    maximize=False,\n",
    "    data_name=\"validation_0\",\n",
    "    metric_name=\"logloss\",)\n",
    "    \n",
    "    model = XGBClassifier(tree_method='hist',\n",
    "                          n_estimators=2000, \n",
    "                          objective='binary:logistic',\n",
    "                          early_stopping_rounds=100, \n",
    "                          enable_categorical=True, \n",
    "                          eval_metric=['logloss', 'auc'],\n",
    "                          n_jobs=4,\n",
    "                          random_state=1337,\n",
    "                          callbacks=[es],\n",
    "                           \n",
    "                          max_depth = 6,)\n",
    "    \n",
    "    model = model.fit(Xtrain, ytrain, \n",
    "                      eval_set=[(Xvalid, yvalid)],\n",
    "                      verbose=50)\n",
    "    \n",
    "    # predict\n",
    "    ypred_proba = model.predict_proba(Xvalid)\n",
    "    ypred = model.predict(Xvalid)\n",
    "    fold_logloss = log_loss(yvalid, ypred_proba)\n",
    "    fold_metric = roc_auc_score(yvalid, ypred_proba[:,1])\n",
    "    \n",
    "    # save\n",
    "    fold_loglosses.append(fold_logloss)\n",
    "    fold_metrics.append(fold_metric)\n",
    "    print(f\"Fold {i+1} Log Loss: {fold_logloss:.5f}, AUC_ROC: {fold_metric:.5f}\")\n",
    "\n",
    "\n",
    "print(f\"\\nOverall Score, logloss: {np.mean(fold_loglosses):.5f}, auc: {np.mean(fold_metrics):.5f}\")\n",
    "summary_list = ['One Hot', str(Xtrain.shape[1]), str(round(np.mean(fold_loglosses),5)), str(round(np.mean(fold_metrics),5))]\n",
    "for i, (k, v) in enumerate(summary_dict.items()):\n",
    "    v.append(summary_list[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd85db4",
   "metadata": {},
   "source": [
    "Let's try target encoding. We need to do the target encoding inside the cv loop to avoid \n",
    "leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5efab26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.25916\tvalidation_0-auc:0.93935\n",
      "[50]\tvalidation_0-logloss:0.15113\tvalidation_0-auc:0.96468\n",
      "[100]\tvalidation_0-logloss:0.14841\tvalidation_0-auc:0.96604\n",
      "[150]\tvalidation_0-logloss:0.14698\tvalidation_0-auc:0.96674\n",
      "[175]\tvalidation_0-logloss:0.14666\tvalidation_0-auc:0.96691\n",
      "Fold 1 Log Loss: 0.14666, AUC_ROC: 0.96691\n",
      "[0]\tvalidation_0-logloss:0.26054\tvalidation_0-auc:0.93907\n",
      "[50]\tvalidation_0-logloss:0.15089\tvalidation_0-auc:0.96520\n",
      "[100]\tvalidation_0-logloss:0.14842\tvalidation_0-auc:0.96645\n",
      "[150]\tvalidation_0-logloss:0.14723\tvalidation_0-auc:0.96705\n",
      "[158]\tvalidation_0-logloss:0.14716\tvalidation_0-auc:0.96709\n",
      "Fold 2 Log Loss: 0.14716, AUC_ROC: 0.96709\n",
      "[0]\tvalidation_0-logloss:0.26036\tvalidation_0-auc:0.93847\n",
      "[50]\tvalidation_0-logloss:0.15241\tvalidation_0-auc:0.96405\n",
      "[100]\tvalidation_0-logloss:0.14968\tvalidation_0-auc:0.96540\n",
      "[150]\tvalidation_0-logloss:0.14844\tvalidation_0-auc:0.96601\n",
      "[181]\tvalidation_0-logloss:0.14808\tvalidation_0-auc:0.96620\n",
      "Fold 3 Log Loss: 0.14808, AUC_ROC: 0.96620\n",
      "[0]\tvalidation_0-logloss:0.25896\tvalidation_0-auc:0.93884\n",
      "[50]\tvalidation_0-logloss:0.15043\tvalidation_0-auc:0.96488\n",
      "[100]\tvalidation_0-logloss:0.14753\tvalidation_0-auc:0.96633\n",
      "[150]\tvalidation_0-logloss:0.14645\tvalidation_0-auc:0.96687\n",
      "[184]\tvalidation_0-logloss:0.14617\tvalidation_0-auc:0.96699\n",
      "Fold 4 Log Loss: 0.14617, AUC_ROC: 0.96699\n",
      "[0]\tvalidation_0-logloss:0.26013\tvalidation_0-auc:0.94045\n",
      "[50]\tvalidation_0-logloss:0.15055\tvalidation_0-auc:0.96521\n",
      "[100]\tvalidation_0-logloss:0.14794\tvalidation_0-auc:0.96646\n",
      "[150]\tvalidation_0-logloss:0.14679\tvalidation_0-auc:0.96699\n",
      "[169]\tvalidation_0-logloss:0.14675\tvalidation_0-auc:0.96703\n",
      "Fold 5 Log Loss: 0.14673, AUC_ROC: 0.96703\n",
      "\n",
      "Overall Score, logloss: 0.14696, auc: 0.96685\n"
     ]
    }
   ],
   "source": [
    "df_y = df_train[['y']].copy()\n",
    "df_X = df_train.drop(columns=['y']).copy()\n",
    "\n",
    "\n",
    "KFOLD = 5\n",
    "kf = KFold(n_splits=KFOLD, shuffle=True, random_state=1337)\n",
    "\n",
    "fold_loglosses = []\n",
    "fold_metrics = []\n",
    "for i,(train_index, valid_index) in enumerate(kf.split(df_X)):\n",
    "    Xtrain = df_X.iloc[train_index]\n",
    "    ytrain = df_y.iloc[train_index]\n",
    "    Xvalid = df_X.iloc[valid_index]\n",
    "    yvalid = df_y.iloc[valid_index]\n",
    "    Xtest = df_test.copy()\n",
    "    \n",
    "    enc = TargetEncoder(cols=cats, \n",
    "                    min_samples_leaf=20, \n",
    "                    smoothing=10).fit(Xtrain, ytrain)\n",
    "    Xtrain = enc.transform(Xtrain)\n",
    "    Xvalid = enc.transform(Xvalid)\n",
    "    Xtest = enc.transform(Xtest)\n",
    "\n",
    "    # XGB    \n",
    "    # Early stopping call back, use to get best model back\n",
    "    es = xgb.callback.EarlyStopping(\n",
    "    rounds=50,\n",
    "    min_delta=1e-3,\n",
    "    save_best=True,\n",
    "    maximize=False,\n",
    "    data_name=\"validation_0\",\n",
    "    metric_name=\"logloss\",)\n",
    "    \n",
    "    model = XGBClassifier(tree_method='hist',\n",
    "                          n_estimators=2000, \n",
    "                          objective='binary:logistic',\n",
    "                          early_stopping_rounds=100, \n",
    "                          enable_categorical=True, \n",
    "                          eval_metric=['logloss', 'auc'],\n",
    "                          n_jobs=4,\n",
    "                          random_state=1337,\n",
    "                          callbacks=[es],\n",
    "                           \n",
    "                          max_depth = 6,)\n",
    "    \n",
    "    model = model.fit(Xtrain, ytrain, \n",
    "                      eval_set=[(Xvalid, yvalid)],\n",
    "                      verbose=50)\n",
    "    \n",
    "    # predict\n",
    "    ypred_proba = model.predict_proba(Xvalid)\n",
    "    ypred = model.predict(Xvalid)\n",
    "    fold_logloss = log_loss(yvalid, ypred_proba)\n",
    "    fold_metric = roc_auc_score(yvalid, ypred_proba[:,1])\n",
    "    \n",
    "    # save\n",
    "    fold_loglosses.append(fold_logloss)\n",
    "    fold_metrics.append(fold_metric)\n",
    "    print(f\"Fold {i+1} Log Loss: {fold_logloss:.5f}, AUC_ROC: {fold_metric:.5f}\")\n",
    "    \n",
    "\n",
    "print(f\"\\nOverall Score, logloss: {np.mean(fold_loglosses):.5f}, auc: {np.mean(fold_metrics):.5f}\")\n",
    "summary_list = ['Target Encoding', str(Xtrain.shape[1]), str(round(np.mean(fold_loglosses),5)), str(round(np.mean(fold_metrics),5))]\n",
    "for i, (k, v) in enumerate(summary_dict.items()):\n",
    "    v.append(summary_list[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a8e9be",
   "metadata": {},
   "source": [
    "Let's try weight of evidence encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12693155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.25916\tvalidation_0-auc:0.93935\n",
      "[50]\tvalidation_0-logloss:0.15113\tvalidation_0-auc:0.96468\n",
      "[100]\tvalidation_0-logloss:0.14841\tvalidation_0-auc:0.96604\n",
      "[150]\tvalidation_0-logloss:0.14698\tvalidation_0-auc:0.96674\n",
      "[175]\tvalidation_0-logloss:0.14666\tvalidation_0-auc:0.96691\n",
      "Fold 1 Log Loss: 0.14666, AUC_ROC: 0.96691\n",
      "[0]\tvalidation_0-logloss:0.26054\tvalidation_0-auc:0.93907\n",
      "[50]\tvalidation_0-logloss:0.15089\tvalidation_0-auc:0.96520\n",
      "[100]\tvalidation_0-logloss:0.14842\tvalidation_0-auc:0.96645\n",
      "[150]\tvalidation_0-logloss:0.14723\tvalidation_0-auc:0.96705\n",
      "[158]\tvalidation_0-logloss:0.14716\tvalidation_0-auc:0.96709\n",
      "Fold 2 Log Loss: 0.14716, AUC_ROC: 0.96709\n",
      "[0]\tvalidation_0-logloss:0.26036\tvalidation_0-auc:0.93847\n",
      "[50]\tvalidation_0-logloss:0.15190\tvalidation_0-auc:0.96429\n",
      "[100]\tvalidation_0-logloss:0.14949\tvalidation_0-auc:0.96552\n",
      "[150]\tvalidation_0-logloss:0.14832\tvalidation_0-auc:0.96613\n",
      "[179]\tvalidation_0-logloss:0.14799\tvalidation_0-auc:0.96629\n",
      "Fold 3 Log Loss: 0.14799, AUC_ROC: 0.96629\n",
      "[0]\tvalidation_0-logloss:0.25896\tvalidation_0-auc:0.93884\n",
      "[50]\tvalidation_0-logloss:0.15043\tvalidation_0-auc:0.96488\n",
      "[100]\tvalidation_0-logloss:0.14753\tvalidation_0-auc:0.96633\n",
      "[150]\tvalidation_0-logloss:0.14645\tvalidation_0-auc:0.96687\n",
      "[184]\tvalidation_0-logloss:0.14617\tvalidation_0-auc:0.96699\n",
      "Fold 4 Log Loss: 0.14617, AUC_ROC: 0.96699\n",
      "[0]\tvalidation_0-logloss:0.26013\tvalidation_0-auc:0.94045\n",
      "[50]\tvalidation_0-logloss:0.15055\tvalidation_0-auc:0.96521\n",
      "[100]\tvalidation_0-logloss:0.14794\tvalidation_0-auc:0.96646\n",
      "[150]\tvalidation_0-logloss:0.14679\tvalidation_0-auc:0.96699\n",
      "[170]\tvalidation_0-logloss:0.14670\tvalidation_0-auc:0.96705\n",
      "Fold 5 Log Loss: 0.14670, AUC_ROC: 0.96705\n",
      "\n",
      "Overall Score, logloss: 0.14694, auc: 0.96687\n"
     ]
    }
   ],
   "source": [
    "df_y = df_train[['y']].copy()\n",
    "df_X = df_train.drop(columns=['y']).copy()\n",
    "Xtest = df_test.copy()\n",
    "\n",
    "KFOLD = 5\n",
    "kf = KFold(n_splits=KFOLD, shuffle=True, random_state=1337)\n",
    "\n",
    "fold_loglosses = []\n",
    "fold_metrics = []\n",
    "for i,(train_index, valid_index) in enumerate(kf.split(df_X)):\n",
    "    Xtrain = df_X.iloc[train_index]\n",
    "    ytrain = df_y.iloc[train_index]\n",
    "    Xvalid = df_X.iloc[valid_index]\n",
    "    yvalid = df_y.iloc[valid_index]\n",
    "    Xtest = df_test.copy()\n",
    "    \n",
    "    enc = WOEEncoder(cols=cats).fit(Xtrain, ytrain)\n",
    "    Xtrain = enc.transform(Xtrain)\n",
    "    Xvalid = enc.transform(Xvalid)\n",
    "    Xtest = enc.transform(Xtest)\n",
    "\n",
    "    # XGB    \n",
    "    # Early stopping call back, use to get best model back\n",
    "    es = xgb.callback.EarlyStopping(\n",
    "    rounds=50,\n",
    "    min_delta=1e-3,\n",
    "    save_best=True,\n",
    "    maximize=False,\n",
    "    data_name=\"validation_0\",\n",
    "    metric_name=\"logloss\",)\n",
    "    \n",
    "    model = XGBClassifier(tree_method='hist',\n",
    "                          n_estimators=2000, \n",
    "                          objective='binary:logistic',\n",
    "                          early_stopping_rounds=100, \n",
    "                          enable_categorical=True, \n",
    "                          eval_metric=['logloss', 'auc'],\n",
    "                          n_jobs=4,\n",
    "                          random_state=1337,\n",
    "                          callbacks=[es],\n",
    "                           \n",
    "                          max_depth = 6,)\n",
    "    \n",
    "    model = model.fit(Xtrain, ytrain, \n",
    "                      eval_set=[(Xvalid, yvalid)],\n",
    "                      verbose=50)\n",
    "    \n",
    "    # predict\n",
    "    ypred_proba = model.predict_proba(Xvalid)\n",
    "    ypred = model.predict(Xvalid)\n",
    "    fold_logloss = log_loss(yvalid, ypred_proba)\n",
    "    fold_metric = roc_auc_score(yvalid, ypred_proba[:,1])\n",
    "    \n",
    "    # save\n",
    "    fold_loglosses.append(fold_logloss)\n",
    "    fold_metrics.append(fold_metric)\n",
    "    print(f\"Fold {i+1} Log Loss: {fold_logloss:.5f}, AUC_ROC: {fold_metric:.5f}\")\n",
    "\n",
    "\n",
    "print(f\"\\nOverall Score, logloss: {np.mean(fold_loglosses):.5f}, auc: {np.mean(fold_metrics):.5f}\")\n",
    "summary_list = ['Weight of Evidence', str(Xtrain.shape[1]), str(round(np.mean(fold_loglosses),5)), str(round(np.mean(fold_metrics),5))]\n",
    "for i, (k, v) in enumerate(summary_dict.items()):\n",
    "    v.append(summary_list[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10542fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coding Algo</th>\n",
       "      <th>Number of Features</th>\n",
       "      <th>Average Logloss</th>\n",
       "      <th>Average ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Label</td>\n",
       "      <td>16</td>\n",
       "      <td>0.14711</td>\n",
       "      <td>0.96677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One Hot</td>\n",
       "      <td>42</td>\n",
       "      <td>0.14774</td>\n",
       "      <td>0.96646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Target Encoding</td>\n",
       "      <td>16</td>\n",
       "      <td>0.14696</td>\n",
       "      <td>0.96685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Weight of Evidence</td>\n",
       "      <td>16</td>\n",
       "      <td>0.14694</td>\n",
       "      <td>0.96687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Coding Algo Number of Features Average Logloss Average ROC-AUC\n",
       "0               Label                 16         0.14711         0.96677\n",
       "1             One Hot                 42         0.14774         0.96646\n",
       "2     Target Encoding                 16         0.14696         0.96685\n",
       "3  Weight of Evidence                 16         0.14694         0.96687"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_summary = pd.DataFrame.from_dict(summary_dict)\n",
    "display(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f384a15",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "### Performance Metrics Summary\n",
    "\n",
    "| Encoding Method | Features | ROC-AUC | Log Loss | Performance Rank |\n",
    "|----------------|----------|---------|----------|------------------|\n",
    "| Weight of Evidence | 16 | 0.96687 | 0.14694 | 1st |\n",
    "| Target Encoding | 16 | 0.96685 | 0.14696 | 2nd |\n",
    "| Label Encoding | 16 | 0.96677 | 0.14711 | 3rd |\n",
    "| One-Hot Encoding | 42 | 0.96646 | 0.14774 | 4th |\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**Weight of Evidence encoding achieved the best overall performance**, demonstrating the value of sophisticated statistical encoding methods that capture the relationship between categorical variables and the target.\n",
    "\n",
    "**Dimensional efficiency matters**: Target-aware encoding methods (WOE and Target Encoding) achieved superior performance with only 16 features, while One-Hot Encoding required 42 features yet delivered the weakest performance.\n",
    "\n",
    "**Performance margins are meaningful**: While absolute differences appear small (0.04% in ROC-AUC), these translate to significant improvements in a dataset of this size, potentially affecting thousands of predictions.\n",
    "\n",
    "### Relust Comparison\n",
    "These results can be compared with the competition results [here](https://www.kaggle.com/competitions/playground-series-s5e8/leaderboard). Where the current leader is at 0.97820. With our best result at 0.96687 there is quite a big difference. Of course our result is a baseline with minimal data processing. Many thing can be done to improve this result such as feature engineering, using different ML models and ensembling.\n",
    "\n",
    "\n",
    "## When to Use Each Encoding Method\n",
    "\n",
    "### Label Encoding\n",
    "**Best for**: \n",
    "- Tree-based algorithms (Random Forest, XGBoost, LightGBM)\n",
    "- Naturally ordinal categorical variables\n",
    "- Memory-constrained environments\n",
    "- Quick prototyping and baseline models\n",
    "\n",
    "**Avoid when**: \n",
    "- Using linear models (introduces false ordinality)\n",
    "- Categories have no natural ordering\n",
    "- High cardinality with sparse categories\n",
    "\n",
    "### One-Hot Encoding\n",
    "**Best for**: \n",
    "- Linear models (Logistic Regression, SVM, Neural Networks)\n",
    "- Low-cardinality categorical variables (< 10-15 categories)\n",
    "- When feature interpretability is crucial\n",
    "- No relationship between category frequency and target\n",
    "\n",
    "**Avoid when**: \n",
    "- High-cardinality variables (creates sparse, high-dimensional data)\n",
    "- Memory or computational resources are limited\n",
    "- Strong correlation exists between category frequency and target\n",
    "\n",
    "### Target Encoding\n",
    "**Best for**: \n",
    "- High-cardinality categorical variables\n",
    "- Strong relationship between categories and target\n",
    "- Tree-based algorithms or neural networks\n",
    "- Competitive machine learning scenarios\n",
    "\n",
    "**Requires careful attention to**: \n",
    "- Data leakage prevention (proper cross-validation implementation)\n",
    "- Overfitting to rare categories (use smoothing/regularization)\n",
    "- Sufficient sample sizes per category\n",
    "\n",
    "### Weight of Evidence Encoding\n",
    "**Best for**: \n",
    "- Binary classification problems\n",
    "- Credit scoring and risk assessment applications\n",
    "- When monotonic relationship with target is desired\n",
    "- Handling rare categories robustly\n",
    "\n",
    "**Particularly effective when**: \n",
    "- Categories have varying sample sizes\n",
    "- Some categories are rare but informative\n",
    "- Model interpretability in business contexts is important\n",
    "\n",
    "## Technical Implementation Notes\n",
    "\n",
    "### Code Quality Assessment\n",
    "The original notebook demonstrates **solid implementation practices**:\n",
    "- Proper cross-validation structure prevents data leakage\n",
    "- Consistent random seeding ensures reproducibility\n",
    "- Appropriate use of early stopping prevents overfitting\n",
    "- Clean separation of encoding logic from model training\n",
    "\n",
    "### Areas for Enhancement\n",
    "- **Error handling**: Add validation for edge cases and missing values\n",
    "- **Hyperparameter tuning**: Could benefit from systematic parameter optimization\n",
    "- **Feature importance analysis**: Understanding which encoded features contribute most\n",
    "- **Statistical significance testing**: Formal testing of performance differences\n",
    "\n",
    "## Conclusions and Recommendations\n",
    "\n",
    "**Primary Recommendation**: For this type of banking/marketing dataset with binary targets, **Weight of Evidence encoding should be the preferred approach**, offering the best balance of performance, interpretability, and robust handling of categorical variables.\n",
    "\n",
    "**For practitioners**: \n",
    "1. **Start with Target Encoding or WOE** for high-cardinality categorical variables in tree-based models\n",
    "2. **Reserve One-Hot Encoding** for linear models or when maintaining feature interpretability is crucial\n",
    "3. **Use Label Encoding** as a robust baseline, especially in tree-based algorithms\n",
    "4. **Always implement target-aware encoding within cross-validation loops** to prevent data leakage\n",
    "\n",
    "**Dataset-specific insights**: The bank marketing dataset's categorical variables exhibit strong relationships with the target variable, making target-aware encoding methods particularly effective. The consistent performance advantage of WOE and Target Encoding suggests these relationships are genuine and valuable for prediction.\n",
    "\n",
    "This analysis reinforces the importance of thoughtful feature engineering in machine learning pipelines and demonstrates that sophisticated encoding methods can provide meaningful performance improvements even with robust algorithms like XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1daf0d7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
